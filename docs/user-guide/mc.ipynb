{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c96f55-5fd4-413a-a16c-3b32adbf8943",
   "metadata": {},
   "source": [
    "# Monte-Carlo propagation of uncertainties\n",
    "\n",
    "This guide introduces the [mc](../reference/index.rst#monte-carlo) sub package for propagation of uncertainties using Monte-Carlo.\n",
    "\n",
    "Monte-Carlo (MC) can propagate uncertainties through most operations including non-differentiable ones and operations that introduce correlations.\n",
    "It does so by sampling new data from an input with uncertainties, performing the desired operation on each sample, and combining the results to compute, e.g., a mean and variance.\n",
    "While this method is powerful, it is very expensive and in practice only applicable to relatively small and well behaved data.\n",
    "\n",
    "Mathematically, the mc package does the following (the examples below will make the terminology more concrete).\n",
    "\n",
    "Given a measured, empirical distribution of random variables $\\mathbf{X}$, we want to compute some parameter $\\theta$ (the result of our operation).\n",
    "We estimate $\\theta$ using an estimator $s$, such that $\\hat{\\theta} = s(\\mathbf{X})$.\n",
    "We assume all $X_i$ to be **independently** distributed with distribution $X_i = P(\\mu_i, \\sigma_i)$, where $\\mu_i$ and $\\sigma_i$ are the mean and standard deviation of $X_i$.\n",
    "We draw $R$ samples ('replicas') from $P$:\n",
    "$$\n",
    "\\mathbf{x}^\\ast = [x_{i_1}^\\ast,\\, \\dots, x_{i_n}^\\ast], \\quad \\text{where} \\quad x_i^\\ast \\sim X_i = P(\\mu_i, \\sigma_i).\n",
    "$$\n",
    "Then we pass each sample to the estimator $s$ to obtain replicas of the target parameter $\\hat{\\theta}^\\ast = s(\\mathbf{x}^\\ast)$.\n",
    "The final results are then, typically, the mean and variance over all $R$ replicas\n",
    "$$\n",
    "\\mu(\\hat{\\theta}^\\ast) = \\frac{1}{R} \\sum_{r=1}^{R}\\,s(\\mathbf{x}^\\ast_r),\\\\\\\\\n",
    "\\text{var}(\\hat{\\theta}^\\ast) = \\frac{1}{R-1} \\sum_{r=1}^{R}\\,{\\big(s(\\mathbf{x}^\\ast_r) - \\mu(\\hat{\\theta}^\\ast)\\big)}^2\n",
    "$$\n",
    "\n",
    "Let's look at some examples to make this more concrete.\n",
    "Suppose we are given some positions and time information and want to compute the corresponding speeds.\n",
    "We will assume that all input data is independently normally distributed.\n",
    "First, generate some dummy data of positions with uncertainties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02603a-a0a8-4152-887e-016b9535fe34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plopp as pp\n",
    "import scipp as sc\n",
    "\n",
    "from scippuncertainty import mc\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ioff()\n",
    "\n",
    "# Format units of variances nicely.\n",
    "sc.units.aliases['m^2/s^2'] = 'm^2/s^2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bb794-a432-4c27-9441-234e0939c8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(381)\n",
    "n = 10\n",
    "x = sc.linspace(\"x\", 1, 2, n)\n",
    "variances = rng.uniform(0.01, 0.05, n)**2\n",
    "pos = sc.DataArray(\n",
    "    sc.array(\n",
    "        dims=[\"x\"],\n",
    "        values=x.values + rng.normal(4.0, np.sqrt(variances)),\n",
    "        variances=variances,\n",
    "        unit=\"m\",\n",
    "    ),\n",
    "    coords={\"x\": x},\n",
    ")\n",
    "pp.plot(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f615f93-680b-4fcf-9274-a2f2c4c9799d",
   "metadata": {},
   "source": [
    "## One input variable\n",
    "\n",
    "Suppose we want to compute some speed from the positions given above and the following time.\n",
    "For now, the time is assumed to be exact, i.e., it has no variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d0628-5847-4f55-9265-219506a86d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time = sc.DataArray(sc.scalar(10.0, unit=\"s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985e00a-1277-4c16-9789-2a22c078aa60",
   "metadata": {},
   "source": [
    "Define a function to compute the speed.\n",
    "This corresponds to the estimator $s$.\n",
    "(You will see below why this returns a `dict`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe16c9b-04ce-4088-9f3d-f8d831d1b89d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_speed(\n",
    "    pos: sc.DataArray, time: sc.DataArray\n",
    ") -> Dict[str, sc.DataArray]:\n",
    "    return {\"speed\": pos / time}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3882f319-2e5c-4083-8272-dad8682d5e2c",
   "metadata": {},
   "source": [
    "Given these times and speed calculation, we could do regular error propagation using Scipp's builtin mechanism.\n",
    "We can use this to check our MC results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266ac9f-4d42-4be8-89c2-e4bb81842b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speed_regular = compute_speed(pos=pos, time=time)[\"speed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1fc62c-bc07-420a-887e-e503795db6a6",
   "metadata": {},
   "source": [
    "Now, in order to compute the uncertainties with MC, we need to create a few helper objects.\n",
    "First, define a sampler.\n",
    "This will be used to draw new samples from the input `pos`.\n",
    "Since we assume normally distributed data, we use a [NormalDenseSampler](../generated/modules/scippuncertainty.mc.sampler.NormalDenseSampler.rst).\n",
    "This defines the distribution $X_i = P(\\mu_i, \\sigma_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975a81d-7b8b-41f5-9793-78208f6701d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_sampler = mc.NormalDenseSampler(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f3cf8-01bc-44c2-a3e6-aa59caa47cff",
   "metadata": {},
   "source": [
    "Next, we need to define how to collect the replicas and compute an output statistic.\n",
    "In this case, we simply want to compute the mean and variance which is implemented by [VarianceAccum](../generated/modules/scippuncertainty.mc.accumulator.VarianceAccum.rst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e9313-c42e-41d0-a72d-82204a183bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accumulator = mc.VarianceAccum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d68fb7-5456-4842-aaf2-1162cf504900",
   "metadata": {},
   "source": [
    "Finally, we can use [mc.run](../generated/modules/scippuncertainty.mc.driver.run.rst) to put everything together and actually perform the MC computation.\n",
    "\n",
    "We pass a `dict` as the samplers that identifies the `pos_sampler` with the `pos` argument of `compute_speed`.\n",
    "The accumulators `dict` defines how to accumulate each output of `compute_speed`.\n",
    "There is only one here, but the accumulators still have to match the name in the `dict` returned by `compute_speed`.\n",
    "Since `time` has no uncertainties, we simply bind our fixed values using `partial`.\n",
    "\n",
    "`n_samples` corresponds to the number of replicas $R$.\n",
    "It is very high in this case because the computation is quite fast.\n",
    "In practice, numbers in the hundreds or low thousands are more feasible.\n",
    "Lastly, we disable progress reporting as this does not work reliably in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f4f2e-e4ea-4ab5-a643-b89b0dfcead8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "results = mc.run(\n",
    "    partial(compute_speed, time=time),\n",
    "    samplers={\"pos\": pos_sampler},\n",
    "    accumulators={\"speed\": accumulator},\n",
    "    n_samples=10_000,\n",
    "    progress=False,\n",
    ")\n",
    "speed_mc = results[\"speed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd1700-0f27-42dc-bce8-729afa1223e0",
   "metadata": {},
   "source": [
    "Now compare the results of the two calculations.\n",
    "It looks like MC and 'regular' uncertainty propagation are in agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7056e81-2da3-4a2f-bcf1-45cc969a2c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp.plot({\"regular\": speed_regular, \"mc\": speed_mc},\n",
    "        ls='-', marker=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6dc7f-2611-4922-a71e-f7dea2a8bc3e",
   "metadata": {},
   "source": [
    "Also compare the relative errors of both results.\n",
    "Again, there is general agreement as expected.\n",
    "However there are some deviations because MC has only a finite precision.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Attention**\n",
    "    \n",
    "Always make sure that your MC has properly converged to the desired precision.\n",
    "See the choosing $R$ section below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c60782-7c64-419a-959a-3efa6b0844f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relative_error(da: sc.DataArray) -> sc.DataArray:\n",
    "    return sc.stddevs(da) / abs(sc.values(da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7073ef-3205-451c-9f3f-eef2d8e7e163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_relative_errors(\n",
    "    data_regular: sc.DataArray, data_mc: sc.DataArray\n",
    ") -> sc.DataArray:\n",
    "    rel_err_regular = relative_error(data_regular)\n",
    "    rel_err_mc = relative_error(data_mc)\n",
    "\n",
    "    fig = plt.figure(figsize=(11, 5.5))\n",
    "    gs = fig.add_gridspec(2, 1, height_ratios=(3, 1), hspace=0)\n",
    "\n",
    "    ax = fig.add_subplot(gs[0])\n",
    "    pp.plot({\"regular\": rel_err_regular, \"mc\": rel_err_mc}, ax=ax)\n",
    "    ax.set_ylabel(r\"$\\sigma_\\mathsf{rel}$\")\n",
    "\n",
    "    ax = fig.add_subplot(gs[1])\n",
    "    pp.plot(rel_err_regular - rel_err_mc, ax=ax, c=\"k\")\n",
    "    ax.set_ylabel(r\"$\\Delta$\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d3bca-d655-4a82-9392-3684262aeab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_relative_errors(speed_regular, speed_mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a8bb5-14c1-4f03-8118-992975c0dbed",
   "metadata": {},
   "source": [
    "## Broadcasting: introduction of correlations\n",
    "\n",
    "This section introduces both the impact of correlations and how to sample from multiple inputs.\n",
    "\n",
    "In the section above, there is no reason to use Monte-Carlo to propagate uncertainties because Scipp's builtin method is correct.\n",
    "So now, let us look at a case where regular uncertainty propagation fails because it cannot account for correlations.\n",
    "A common source of correlations in Scipp is broadcasting of data with variances.\n",
    "To this end, define a new `time` data array, this time with variances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e7275-bcc7-40da-b5f6-78b0db1fae92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_with_var = sc.DataArray(sc.scalar(10.0, variance=0.1**2, unit=\"s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27245598-5f8c-446f-b9d4-424a4bb9657c",
   "metadata": {},
   "source": [
    "Modify the speed function to also compute the average speed as this is sensitive to correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf9c95e-f2f2-44d1-9090-e39a9778c7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_average_speed(\n",
    "    pos: sc.DataArray, time: sc.DataArray\n",
    ") -> Dict[str, sc.DataArray]:\n",
    "    speed = pos / time\n",
    "    return {\"speed\": speed, \"average_speed\": sc.mean(speed)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140b3ce-7c20-4d1a-b164-c8bcf52cd45f",
   "metadata": {},
   "source": [
    "Trying to call `compute_average_speed` with this new time raises an exception.\n",
    "This is because `time` needs to be broadcast to match the shape of `pos`.\n",
    "In other words, the same time value needs to be divided into every single position value.\n",
    "This would introduce correlations between the resulting speeds as they would all depend on the same time and its uncertainty.\n",
    "Scipp cautiously prevents such broadcasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317e5fc-bc10-415d-bdef-5345e4d650a2",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "compute_average_speed(pos=pos, time=time_with_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f916fa13-e098-4cab-bee2-c58be2893eef",
   "metadata": {},
   "source": [
    "For this guide, let us bypass Scipp's check by using an explicit broadcast.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Warning**\n",
    "    \n",
    "In practice, you have to check whether this is valid for your concrete use case!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e5d5d-404a-44bd-8a0f-3d361ffec33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = compute_average_speed(\n",
    "    pos=pos, time=sc.broadcast(time_with_var, sizes=pos.sizes).copy()\n",
    ")\n",
    "speed_regular = results[\"speed\"]\n",
    "average_speed_regular = results[\"average_speed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085075fb-ffe3-45be-82e9-00a2f7ba6b45",
   "metadata": {},
   "source": [
    "Now, set up the MC run in a similar way to before.\n",
    "First, define samplers for both position and time.\n",
    "This defines the probability distribution as the concatenation $X = [P_{\\text{pos}}(\\mu, \\sigma), P_{\\text{time}}(\\mu, \\sigma)]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116bacef-ca3f-4690-a5ec-329d07880e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_sampler = mc.NormalDenseSampler(pos)\n",
    "time_sampler = mc.NormalDenseSampler(time_with_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd74b72-edec-4e8f-b326-dd12cf6c6f76",
   "metadata": {},
   "source": [
    "Define accumulators for both outputs of `compute_average_speed`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccfdf04-f1f1-4bdf-ac36-b39d5ddae6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speed_accumulator = mc.VarianceAccum()\n",
    "average_speed_accumulator = mc.VarianceAccum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6ada57-a442-4592-973a-abaf495ccf71",
   "metadata": {},
   "source": [
    "And then perform the MC computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ae1c2-748e-4fa2-9a3b-38e548272d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = mc.run(\n",
    "    compute_average_speed,\n",
    "    samplers={\"pos\": pos_sampler, \"time\": time_sampler},\n",
    "    accumulators={\"speed\": speed_accumulator,\n",
    "                  \"average_speed\": average_speed_accumulator},\n",
    "    n_samples=10_000,\n",
    "    progress=False,\n",
    ")\n",
    "speed_mc = results[\"speed\"]\n",
    "average_speed_mc = results[\"average_speed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b0865-7ad8-4487-bdf4-3f71a7f21a9a",
   "metadata": {},
   "source": [
    "We can inspect the relative error of the speed per position as before.\n",
    "Again, MC and regular uncertainty propagation mostly agree.\n",
    "(The larger discrepancies compared to before are due to the larger spread of samples in MC due to the variance of `time`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7bc3f-2d0a-424c-b737-8a0726b00e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_relative_errors(speed_regular, speed_mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815e91a-7e1e-4ed3-b615-992750b10f88",
   "metadata": {},
   "source": [
    "The above is expected because the induced correlations do not show up in the variances of the speed.\n",
    "We need to look at a quantity that is sensitive to those correlations.\n",
    "For example, the average speed as it is summed over all component speeds.\n",
    "\n",
    "Indeed, MC produces a significantly larger standard deviation than regular propagation of uncertainties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc220d4-cbf5-4de6-bfd1-08b5cffa1f70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Regular: {average_speed_regular.data:c}\")\n",
    "print(f\"MC:      {average_speed_mc.data:c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a0afa-9c95-4ae2-b0b2-e315963a388d",
   "metadata": {},
   "source": [
    "## Computing the covariance matrix\n",
    "\n",
    "We can also compute the covariance matrix directly.\n",
    "This lets us inspect correlations of the results on more general grounds.\n",
    "\n",
    "Again, define samplers for position and time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f408d-a7ac-4702-a142-89ec376995bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_sampler = mc.NormalDenseSampler(pos)\n",
    "time_sampler = mc.NormalDenseSampler(time_with_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5219912-186b-4be2-9ce8-06c6b8ce1241",
   "metadata": {},
   "source": [
    "Then, define an accumulator for the covariance in addition to the variance.\n",
    "\n",
    "Note that Scipp has no native way of encoding matrices.\n",
    "So we work around this by storing a 2D array with artificial dimension labels (`dims`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d3bdc-ff0a-41c6-bd20-93bff4210d18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variance_accumulator = mc.VarianceAccum()\n",
    "covariance_accumulator = mc.CovarianceAccum(dims=(\"x0\", \"x1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0ff44-fd4b-43b8-a68e-bb9cd354bc1b",
   "metadata": {},
   "source": [
    "In order to use this, we need to return an additional result from our speed function.\n",
    "Since we want both the mean+variance and covariance matrix of the same result, the speed, we simply return it twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd90b82-9ead-4d1f-9041-022bf302f95e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_speed_cov(\n",
    "    pos: sc.DataArray, time: sc.DataArray\n",
    ") -> Dict[str, sc.DataArray]:\n",
    "    speed = pos / time\n",
    "    return {\"speed\": speed, \"speed_cov\": speed}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce32218-55a5-49c5-9a36-08c311ad3b39",
   "metadata": {},
   "source": [
    "Now, run the MC calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b2a82-0dd7-4f90-9517-8ae54d4974a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = mc.run(\n",
    "    compute_speed_cov,\n",
    "    samplers={\"pos\": pos_sampler, \"time\": time_sampler},\n",
    "    accumulators={\"speed\": variance_accumulator,\n",
    "                  \"speed_cov\": covariance_accumulator},\n",
    "    n_samples=10_000,\n",
    "    progress=False,\n",
    ")\n",
    "speed_mc = results[\"speed\"]\n",
    "speed_cov = results[\"speed_cov\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0fdeb-cc5f-402a-851a-cd1ac9d66f61",
   "metadata": {},
   "source": [
    "`speed_mc` is the same as before.\n",
    "And `speed_cov` holds the variance-covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f214c3a-c08c-4cf7-a34e-f9dd1ae2e007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp.plot(speed_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe86af-8dbd-455d-b9d4-4539469009be",
   "metadata": {},
   "source": [
    "The variance-covariance matrix is difficult to interpret.\n",
    "So compute the linear correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fc711-b0c4-4168-8e08-4c6de91e3f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scippuncertainty import pearson_correlation\n",
    "\n",
    "pp.plot(pearson_correlation(speed_cov), vmin=0.0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9f58f-e2f9-49a7-a25c-6803c1086964",
   "metadata": {},
   "source": [
    "We can see that there are significant correlations between speeds, characterized by off-diagonal elements being close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8725c-7aff-451b-a9d4-adc065b281ca",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Caveats\n",
    "\n",
    "Monte-Carlo is a big gun and should only be used as a last resort.\n",
    "It requires **large computational resources**, time and memory.\n",
    "So much so that a supercomputer is required for many non-trivial problems.\n",
    "It is generally advisable to look for different solutions.\n",
    "It may, for example, be possible to compute variance-covariance matrices analytically or use an automatic differentiation library.\n",
    "\n",
    "Monte-Carlo is a random process and as such only produces **estimates** of uncertainties.\n",
    "And the uncertainty of those estimates usually reduces as $1 / \\sqrt{R}$.\n",
    "This makes MC prohibitively expensive if the estimated distribution has a wide spread.\n",
    "For the examples above, if the input variables for position and time had standard deviations larger than a few percent, the MC results would be too noisy to be useful.\n",
    "\n",
    "### Choosing $R$\n",
    "\n",
    "$R$ needs to be large enough for MC to converge.\n",
    "Empirically, convergence can be determined by running MC with a given $R$, say 100, and store the result.\n",
    "Then, run it again with a larger $R$, say 1000, and check if there is a significant difference in the results.\n",
    "\n",
    "Anecdotally, an $R$ in the hundreds is often sufficient.\n",
    "However, as mentioned above, MC converges rather slowly as $1 / \\sqrt{R}$.\n",
    "So some cases will require much larger $R$s.\n",
    "\n",
    "### Random number generators and seeding\n",
    "\n",
    "By default, `mc.run` constructs its own random number generator (RNG) and seeds it from the system RNG.\n",
    "If you want to make the process reproducible, you can pass a concrete seed via the `seed` argument.\n",
    "See the documentation of [mc.run](../generated/modules/scippuncertainty.mc.driver.run.rst) for details.\n",
    "\n",
    "`mc.run` also sends a log message with the seed it uses.\n",
    "We can see it by configuring the logger, e.g., using the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83e049-2ee5-4bfd-b58e-7b467c09c22a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from scippuncertainty.logging import get_logger\n",
    "\n",
    "handler = logging.StreamHandler()\n",
    "handler.setLevel(\"INFO\")\n",
    "get_logger().addHandler(handler)\n",
    "get_logger().setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00c433-dd47-4947-9fbb-173c96ad0462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = mc.run(\n",
    "    compute_average_speed,\n",
    "    samplers={\"pos\": pos_sampler, \"time\": time_sampler},\n",
    "    accumulators={\"speed\": speed_accumulator,\n",
    "                  \"average_speed\": average_speed_accumulator},\n",
    "    n_samples=10,\n",
    "    progress=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603de723-cd46-47df-a188-2974adb1fb4d",
   "metadata": {},
   "source": [
    "### Multi-threading\n",
    "\n",
    "`mc.run` can use multiple threads where each thread processes a subset of samples.\n",
    "Simply set the desired number of threads with the `n_threads` argument.\n",
    "\n",
    "But bear in mind that this uses Python's threads.\n",
    "So you will only see a speedup if your function spends a lot of time in code that releases the global interpreter lock (GIL).\n",
    "Most functions in Scipp do this.\n",
    "However, since the data in the examples here is so small, multi threading does not improve performance.\n",
    "\n",
    "Note further that most functions in Scipp are internally multi threaded.\n",
    "So you should make sure not to oversubscribe your CPU.\n",
    "\n",
    "### Skipping samples\n",
    "\n",
    "Monte-Carlo can sometimes produce samples that our operation cannot handle, for example because a sample contains a 0 or a fit fails to converge.\n",
    "It is possible to skip those samples.\n",
    "\n",
    "For example, suppose that we had a function that requires positive inputs like `sqrt`.\n",
    "We can detect negative inputs and return `mc.SkipSample` instead of actual data.\n",
    "This instructs `mc.run` to ignore this sample and carry on with the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c97b9-a21d-40cb-82f2-2fd820a085c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def skipping_op(da: sc.DataArray) -> Dict[str, sc.DataArray]:\n",
    "    if sc.any(da < sc.scalar(0)).value:\n",
    "        return mc.SkipSample\n",
    "    return {\"sqrt\": sc.sqrt(da)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e29ab4c-7aed-4ca7-80ce-79477b6bdb6d",
   "metadata": {},
   "source": [
    "Note, though, failed samples still count towards `n_samples`.\n",
    "So the actual number of samples used for the output variance can be different.\n",
    "To find out, look at the `\"n_samples\"'` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ebb5e-dc14-4706-9fff-9e89b0446552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data with large variances s.t. some samples are negative.\n",
    "n = 10\n",
    "x = sc.linspace(\"x\", 1, 2, n)\n",
    "da = sc.DataArray(\n",
    "    sc.array(\n",
    "        dims=[\"x\"],\n",
    "        values=x.values,\n",
    "        variances=rng.uniform(0.1, 1.0, n) ** 2,\n",
    "    ),\n",
    "    coords={\"x\": x},\n",
    ")\n",
    "\n",
    "results = mc.run(\n",
    "    skipping_op,\n",
    "    samplers={\"da\": mc.NormalDenseSampler(da)},\n",
    "    accumulators={\"sqrt\": mc.VarianceAccum()},\n",
    "    n_samples=100,\n",
    "    progress=False,\n",
    ")\n",
    "results[\"sqrt\"].attrs[\"n_samples\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
